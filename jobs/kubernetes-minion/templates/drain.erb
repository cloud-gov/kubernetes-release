#!/bin/bash

# Drain and cordon node

set -e
set -x

PATH=$PATH:/var/vcap/packages/kubernetes/bin

LOG_DIR=/var/vcap/sys/log/kubernetes-minion
LOG_FILE=${LOG_DIR}/drain.log

mkdir -p ${LOG_DIR}

exec 3>&1
exec 1>> ${LOG_FILE}
exec 2>> ${LOG_FILE}

# Choose first non-self address; using self fails if etcd drain script runs first
# the drain script will succeed after 5m regardless due to fixes in 2019.
<% addr = spec.networks.to_h.values.first.ip %>
API_HOST=<%= p('apiserver.hosts').first %>
NODE_HOST=<%= addr %>

MAX_POD_RETRIES=30

api_up() {
  set +e
  nc -vz $API_HOST 8080
  rc=$?
  set -e

  return $rc
}

ready_wait() {
  POD_RETRIES=0

  set +e
  while true; do
    api_up
    if [ $? -ne 0 ]; then
      logger -t error "[BOSH Kubernetes Drain] Could not reach API host ${API_HOST}"
    elif [ -z "$(kubectl -s http://${API_HOST}:8080 get pods -o 'jsonpath={.items[*].status.containerStatuses[?(@.ready==false)].name}')" ]; then
      break
    fi

    if [ ${POD_RETRIES} -gt ${MAX_POD_RETRIES} ]; then
      logger -t error "[BOSH Kubernetes Drain] Kubernetes pods not in a running state, and did not come back while waiting"
      break
    else
      POD_RETRIES=$((POD_RETRIES + 1))
    fi

    sleep 10
  done

  set -e

  return 0
}

# make sure the cluster is in a good state before we start draining
ready_wait

# only attempt drain if we can communicate with the api host
if api_up; then
  QUERY="{range .items[?(.metadata.labels.kubernetes\.io/hostname==\"${NODE_HOST}\")]} {.metadata.name} {end}"
  NODE=$(kubectl -s http://${API_HOST}:8080 get nodes -o jsonpath="${QUERY}")

  for i in {1..5}; do
    kubectl -s http://${API_HOST}:8080 drain ${NODE} --force --ignore-daemonsets --delete-local-data && break || sleep 5
  done
fi

# wait for the cluster to return to a good state after draining before we call this done
ready_wait

echo 0 >&3
exit 0
